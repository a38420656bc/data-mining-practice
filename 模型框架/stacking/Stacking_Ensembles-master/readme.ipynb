{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特点\n",
    "（1）方便扩展，比如扩展Sklearn,Keras,CatBoost等工具（只需继承stacking_classifier中的Classifier类，并实现相应方法即可）；  \n",
    "（2）可以构建很深，很复杂的stacking结构  \n",
    "（3）支持离散变量（为了方便lightgbm,catboost）  \n",
    "（4）支持并行/并发训练 \n",
    "\n",
    "接下来，我在手写数值识别上演示api使用示例：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stacking_classifier import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X, y = digits['data'], digits['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一.基本分类器的使用\n",
    "这里所有的分类器都需要实现Classifier类的接口，如果你是使用的Sklearn风格的分类器，只需要做如下操作即可，stacking_classifier中默认封装了SVMClassifier,RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,BaggingClassifier,LogisticRegression,NaiveBayesClassifier,LightGBMClassifier,CatBoostClassifier等分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(SklearnClassifier):\n",
    "    def __init__(self, train_params=None, subsample_features_rate=None, subsample_features_indices=None,\n",
    "                 categorical_feature_indices=None, n_jobs=1):\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        SklearnClassifier.__init__(self, train_params, LogisticRegression, subsample_features_rate,\n",
    "                                   subsample_features_indices, categorical_feature_indices, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454545086848583\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.build_model()\n",
    "classifier.fit(X_train, y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二.KFolds_Classifier_Training_Wrapper包装器的使用\n",
    "```KFolds_Classifier_Training_Wrapper```可以将数据切分成```k_fold```份，并训练```k_fold```个分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9364765325701125\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier = KFolds_Classifier_Training_Wrapper(classifier,k_fold=5)#这里封装一下即可，默认k_fold=5\n",
    "classifier.build_model()\n",
    "classifier.fit(X_train, y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9403683409907246\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "#KFolds_Classifier_Training_Wrapper也可以嵌套封装，这样下面就有25个基分类器\n",
    "classifier = KFolds_Classifier_Training_Wrapper(KFolds_Classifier_Training_Wrapper(classifier))\n",
    "classifier.build_model()\n",
    "classifier.fit(X_train, y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三.StackingClassifier分类器的使用\n",
    "```StackingClassifier```中的基分类器和元分类器可以是任意继承了Classifier类的子类，由于```KFolds_Classifier_Training_Wrapper```以及```StackingClassifier```都继承了```Classifier```类，所以意味着你可以任意嵌套..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9286341275747775\n"
     ]
    }
   ],
   "source": [
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        BaggingClassifier(),\n",
    "        SVMClassifier(),\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression(),\n",
    "    force_cv=False#默认为True,会对base_classifiers，meta_classifier进行KFolds_Classifier_Training_Wrapper包装\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_train, train_y=y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9566932316459729\n"
     ]
    }
   ],
   "source": [
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        BaggingClassifier(),\n",
    "        SVMClassifier(),\n",
    "        StackingClassifier(\n",
    "            base_classifiers=[\n",
    "                LogisticRegression(),\n",
    "                RandomForestClassifier(),\n",
    "            ],\n",
    "            meta_classifier=GradientBoostingClassifier(),\n",
    "        )\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression(),\n",
    "    base_k_fold=5,#基分类器分拆份数,force_cv=True时生效，\n",
    "    meta_k_fold=5,#元分类器分拆份数,force_cv=True时生效，\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_train, train_y=y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四.随机/指定选择训练和预测的feature\n",
    "可以随机选择，通过```subsample_features_indices```指定选择训练的feature,```subsample_features_rate```随机选择训练的feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9568685122123126\n"
     ]
    }
   ],
   "source": [
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        RandomForestClassifier(subsample_features_indices=[1,4,7,8]),#指定只使用第1,4,7,8列特征用于训练和预测,上层的参数不会覆盖此参数\n",
    "        AdaBoostClassifier(subsample_features_rate=0.1),#随机选择10%的特征用于训练和预测,上层的参数不会覆盖此参数\n",
    "        BaggingClassifier(),\n",
    "        SVMClassifier(),\n",
    "        StackingClassifier(\n",
    "            base_classifiers=[\n",
    "                LogisticRegression(),\n",
    "                RandomForestClassifier(),\n",
    "            ],\n",
    "            meta_classifier=GradientBoostingClassifier(),\n",
    "        )\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression(),\n",
    "    subsample_features_rate=0.5#该参数会向下传递到最底层的所有未指定subsample_features_rate参数的分类器，subsample_features_indices同理\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_train, train_y=y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五.支持离散变量的输入\n",
    "这里为了方便lightgbm,catboost操作而支持离散变量类型,注意：  \n",
    "（1）**必须在最顶层指定str/object类型的变量**（这样底层不支持str/object类型的分类器才能过滤掉这些特征）；  \n",
    "（2）lightgbm不支持'x','y','z'这种类型的离散变量，只支持‘1’,'2','3'或者int/float类型的离散变量，所以有时需要单独指定；  \n",
    "（3）如果指定了```categorical_feature_indices```参数，```subsample_features_rate,subsample_features_indices```退化为只对剩余的非```categorical_feature_indices```特征生效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为原始数据添加两列：一列是数值的字符串，一列是随意的字符串\n",
    "import numpy as np\n",
    "new_column = np.asarray(['1'] * 1797)\n",
    "new_column2 = np.asarray(['x'] * 1797)\n",
    "X_new = np.concatenate([X, new_column.reshape(1797, 1), new_column2.reshape(1797, 1)], axis=1)\n",
    "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_new, y, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0.0', '0.0', '10.0', '13.0', '9.0', '1.0', '0.0', '0.0', '0.0',\n",
       "        '2.0', '16.0', '7.0', '10.0', '8.0', '0.0', '0.0', '0.0', '0.0',\n",
       "        '12.0', '12.0', '7.0', '11.0', '0.0', '0.0', '0.0', '3.0',\n",
       "        '16.0', '16.0', '16.0', '7.0', '0.0', '0.0', '0.0', '0.0', '5.0',\n",
       "        '8.0', '12.0', '10.0', '1.0', '0.0', '0.0', '0.0', '0.0', '0.0',\n",
       "        '0.0', '11.0', '7.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0',\n",
       "        '3.0', '15.0', '0.0', '0.0', '0.0', '11.0', '16.0', '16.0',\n",
       "        '16.0', '8.0', '0.0', '1', 'x']], dtype='<U32')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952295719121581\n"
     ]
    }
   ],
   "source": [
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        RandomForestClassifier(subsample_features_indices=[1,4,7,8]),\n",
    "        AdaBoostClassifier(subsample_features_rate=0.1),\n",
    "        LogisticRegression(),\n",
    "        LightGBMClassifier(categorical_feature_indices=[64]),#第65列特征为'x','y'类型，ligthgbm底层不支持\n",
    "        CatBoostClassifier(train_params={'depth': 3, 'iterations': 50}),#若不自定义，由顶层传下来的categorical_feature_indices覆盖\n",
    "        StackingClassifier(\n",
    "            base_classifiers=[\n",
    "                LogisticRegression(),\n",
    "                RandomForestClassifier(),\n",
    "            ],\n",
    "            meta_classifier=GradientBoostingClassifier(),\n",
    "        )\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression(),\n",
    "    subsample_features_rate=0.5,\n",
    "    categorical_feature_indices=[64,65],\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_new_train, train_y=y_new_train)\n",
    "p_test = classifier.predict(X_new_test)\n",
    "print(f1_score(y_new_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六.超参设置\n",
    "超参的设置通过```train_params```传入，具体参数的命名与底层封装的分类器一致，比如...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9551347109139192\n"
     ]
    }
   ],
   "source": [
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        RandomForestClassifier(subsample_features_indices=[1,4,7,8],train_params={'n_estimators':200}),\n",
    "        AdaBoostClassifier(subsample_features_rate=0.1),\n",
    "        LogisticRegression(train_params={'penalty':'l2','C':1.0}),\n",
    "        LightGBMClassifier(),\n",
    "        CatBoostClassifier(train_params={'depth': 3, 'iterations': 50}),\n",
    "        StackingClassifier(\n",
    "            base_classifiers=[\n",
    "                LogisticRegression(train_params={'C':2.0}),\n",
    "                RandomForestClassifier(),\n",
    "            ],\n",
    "            meta_classifier=GradientBoostingClassifier(),\n",
    "        )\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression(),\n",
    "    subsample_features_rate=0.5,\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_train, train_y=y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 七.自定义分类器\n",
    "这里使用Keras实现MLP来演示，由于Keras不是Sklearn风格的api，所以需要继承Classifier类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "class SimpleMLPClassifer(Classifier):\n",
    "    def __init__(self, train_params=None, subsample_features_rate=None, subsample_features_indices=None,\n",
    "                 categorical_feature_indices=None):\n",
    "        \"\"\"\n",
    "        :param train_params:\n",
    "        \"\"\"\n",
    "        Classifier.__init__(self, train_params, subsample_features_rate,\n",
    "                                   subsample_features_indices, categorical_feature_indices)\n",
    "        self._check_params()\n",
    "\n",
    "    def _check_params(self):\n",
    "        if 'input_num' not in self.train_params:\n",
    "            raise RuntimeError('no input_num param in train_params!')\n",
    "        if 'class_num' not in self.train_params:\n",
    "            raise RuntimeError('no class_num param in train_params!')\n",
    "        if 'batch_size' not in self.train_params:\n",
    "            self.train_params['batch_size'] = 64\n",
    "        if 'epochs' not in self.train_params:\n",
    "            self.train_params['epochs'] = 5\n",
    "        if 'shuffle' not in self.train_params:\n",
    "            self.train_params['shuffle'] = True\n",
    "        if 'validation_split' not in self.train_params:\n",
    "            self.train_params['validation_split'] = 0.05\n",
    "\n",
    "    def build_model(self):\n",
    "        self.classifier_model = Sequential()\n",
    "        self.classifier_model.add(Dense(512, input_shape=(self.train_params['input_num'],)))\n",
    "        self.classifier_model.add(Activation('relu'))\n",
    "        self.classifier_model.add(Dropout(0.5))\n",
    "        self.classifier_model.add(Dense(self.train_params['class_num']))\n",
    "        self.classifier_model.add(Activation('softmax'))\n",
    "        self.classifier_model.compile(loss='categorical_crossentropy',\n",
    "                                      optimizer='adam',\n",
    "                                      metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        self.classifier_model.fit(x=train_x, y=utils.to_categorical(train_y, self.train_params['class_num']),\n",
    "                                  batch_size=self.train_params['batch_size'], epochs=self.train_params['epochs'],\n",
    "                                  validation_split=self.train_params['validation_split'],\n",
    "                                  shuffle=self.train_params['shuffle'],\n",
    "                                  verbose=False)\n",
    "\n",
    "    def predict_categorical(self, test_x):\n",
    "        categorical_labels = self.classifier_model.predict(test_x, batch_size=test_x.shape[0])\n",
    "        new_categorical_result = np.zeros(shape=categorical_labels.shape)\n",
    "        for index in range(0, len(categorical_labels)):\n",
    "            categorical_label = categorical_labels[index].tolist()\n",
    "            maxvalue_index = categorical_label.index(max(categorical_label))\n",
    "            new_categorical_result[index][maxvalue_index] = 1\n",
    "        return new_categorical_result\n",
    "\n",
    "    def predict(self, test_x):\n",
    "        p_categorical_probas = self.predict_categorical_proba(test_x)\n",
    "        result = []\n",
    "        for categorical_proba in p_categorical_probas:\n",
    "            categorical_proba = categorical_proba.tolist()\n",
    "            result.append(categorical_proba.index(max(categorical_proba)))\n",
    "        return np.asarray(result)\n",
    "\n",
    "    def predict_proba(self, test_x):\n",
    "        return self.classifier_model.predict_proba(test_x, batch_size=test_x.shape[0])\n",
    "\n",
    "    def predict_categorical_proba(self, test_x):\n",
    "        probas = self.classifier_model.predict_proba(test_x)\n",
    "        _, col = probas.shape\n",
    "        if col > 1:\n",
    "            return probas\n",
    "        else:\n",
    "            return np.asarray([[1 - proba, proba] for proba in probas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9552219914166459\n"
     ]
    }
   ],
   "source": [
    "#然后就可以嵌入到Stacking中了\n",
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        LightGBMClassifier(),\n",
    "        CatBoostClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        BaggingClassifier(),\n",
    "        SVMClassifier(),\n",
    "        StackingClassifier(\n",
    "            base_classifiers=[\n",
    "                SimpleMLPClassifer(train_params={'input_num':64,'class_num':10}),#比如放这儿\n",
    "                RandomForestClassifier(),\n",
    "            ],\n",
    "            meta_classifier=GradientBoostingClassifier(),\n",
    "        )\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression()\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_train, train_y=y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 八.并行/并发训练\n",
    "在Linux中采用多进程并行的方式训练，在Windows中采用多线程并发的方式训练，目前仅在Windows中做过简单测试，能比串行训练提速70%+左右（视具体Stacking结构的不同，提速效率也不一样，不建议将meta_classifier定义为复杂的结构，这部分没有做过多优化），使用方式很简单，在最顶层设置```n_jobs=-1```即可，该模块后面还会持续优化..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9576968080725516\n"
     ]
    }
   ],
   "source": [
    "classifier = StackingClassifier(\n",
    "    base_classifiers=[\n",
    "        RandomForestClassifier(subsample_features_indices=[1,4,7,8],train_params={'n_estimators':200}),\n",
    "        AdaBoostClassifier(subsample_features_rate=0.1),\n",
    "        LogisticRegression(train_params={'penalty':'l2','C':1.0}),\n",
    "        LightGBMClassifier(),\n",
    "        CatBoostClassifier(train_params={'depth': 3, 'iterations': 50}),\n",
    "        StackingClassifier(\n",
    "            base_classifiers=[\n",
    "                LogisticRegression(train_params={'C':2.0}),\n",
    "                RandomForestClassifier(),\n",
    "            ],\n",
    "            meta_classifier=GradientBoostingClassifier(),\n",
    "        )\n",
    "    ],\n",
    "    meta_classifier=LogisticRegression(),\n",
    "    subsample_features_rate=0.5,\n",
    "    n_jobs=-1#这里\n",
    ")\n",
    "classifier.build_model()\n",
    "classifier.fit(train_x=X_train, train_y=y_train)\n",
    "p_test = classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 九.模型保存与加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "classifier.save_model('stacking.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载\n",
    "new_classifier=Classifier.load_model('stacking.model')#注意是Classifier类，不是classifier对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9576968080725516\n"
     ]
    }
   ],
   "source": [
    "p_test = new_classifier.predict(X_test)\n",
    "print(f1_score(y_test, p_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 十.回归\n",
    "回归的操作与Classifier类似，不再赘述，下面列一下对应关系：  \n",
    "stacking_classifier->stacking_regressor   \n",
    "Classifier->Regressor  \n",
    "SklearnClassifier->SklearnRegressor  \n",
    "KFolds_Classifier_Training_Wrapper->KFolds_Regressor_Training_Wrapper  \n",
    "StackingClassifier->StackingRegressor  \n",
    "\n",
    "```subsample_features_rate,subsample_features_indices,categorical_feature_indices,n_jobs```的相关内容还未在回归中实现，后续更新..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
