{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "colab": {
      "name": "baseline1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "biNlu4wcvd7v",
        "colab_type": "code",
        "outputId": "b60fdef3-8582-494e-fdc6-801f8b3d8e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.6-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6_tyRn8vi2o",
        "colab_type": "code",
        "outputId": "50c07802-c060-4e0c-9a23-ffef5047e43c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!mkdir -p GuodongDrive\n",
        "!google-drive-ocamlfuse GuodongDrive\n",
        "import os\n",
        "os.chdir(\"GuodongDrive\")\n",
        "os.chdir(\"ccf/carsale\")\n",
        "!ls"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0825ccf_carsale.csv\t   ccf_car_sales082601.csv  submit_example.csv\n",
            "0825ccf_car_sales_lgb.csv  ccf_car_sales082602.csv  train_sales_data.csv\n",
            "baseline082601.ipynb\t   ccf_car_sales.csv\t    train_search_data.csv\n",
            "baseline1_copy.ipynb\t   ccf_car_sales_lgb.csv    train_user_reply_data.csv\n",
            "baseline1.ipynb\t\t   evaluation_public.csv    Untitled0.ipynb\n",
            "baseline2.ipynb\t\t   GuodongDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHBI4gOpvSob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import datetime\n",
        "import time\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpFj_54s9ViS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hcNfGgmbesW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse(true_label,pre_label):\n",
        "    squaredError = (true_label - pre_label)*(true_label - pre_label)\n",
        "    return sum(squaredError) / len(squaredError)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk91hVDSvSou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sales  = pd.read_csv('train_sales_data.csv')\n",
        "train_search = pd.read_csv('train_search_data.csv')\n",
        "train_user   = pd.read_csv('train_user_reply_data.csv')\n",
        "\n",
        "evaluation_public = pd.read_csv('evaluation_public.csv')\n",
        "submit_example    = pd.read_csv('submit_example.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWmn2e7D9Grm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRDpG1jJvSo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1_12    = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==12), 'salesVolume'].values\n",
        "m1_11    = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==11), 'salesVolume'].values\n",
        "m1_10    = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==10), 'salesVolume'].values\n",
        "m1_09    = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==9) , 'salesVolume'].values\n",
        "m1_08    = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==8) , 'salesVolume'].values\n",
        "\n",
        "m1_12_volum = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==12), 'salesVolume'].values * m1_12\n",
        "m1_11_volum = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==11), 'salesVolume'].values * m1_11\n",
        "m1_10_volum = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==10), 'salesVolume'].values * m1_10\n",
        "m1_09_volum = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==9) , 'salesVolume'].values * m1_09\n",
        "m1_08_volum = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==8) , 'salesVolume'].values * m1_08\n",
        "\n",
        "evaluation_public.loc[evaluation_public.regMonth==1, 'forecastVolum'] =  m1_12_volum/2 + m1_11_volum/4 + m1_10_volum/8 + m1_09_volum/16 + m1_08_volum/16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-hmpkyVvSpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m16_1_2  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==2) , 'salesVolume'].values\n",
        "m16_1_3  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==3) , 'salesVolume'].values\n",
        "m16_1_4  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==4) , 'salesVolume'].values\n",
        "m16_1_5  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==5) , 'salesVolume'].values\n",
        "\n",
        "m16_2_3  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==3) , 'salesVolume'].values\n",
        "m16_2_4  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==4) , 'salesVolume'].values\n",
        "m16_2_5  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==5) , 'salesVolume'].values\n",
        "m16_2_6  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==6) , 'salesVolume'].values\n",
        "\n",
        "m16_3_4  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==4) , 'salesVolume'].values\n",
        "m16_3_5  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==5) , 'salesVolume'].values\n",
        "m16_3_6  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==6) , 'salesVolume'].values\n",
        "m16_3_7  = train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2016)&(train_sales.regMonth==7) , 'salesVolume'].values\n",
        "\n",
        "m17_1_2  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==2) , 'salesVolume'].values\n",
        "m17_1_3  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values\n",
        "m17_1_4  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==4) , 'salesVolume'].values\n",
        "m17_1_5  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==5) , 'salesVolume'].values\n",
        "\n",
        "\n",
        "m17_2_3  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values\n",
        "m17_2_4  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==4) , 'salesVolume'].values\n",
        "m17_2_5  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==5) , 'salesVolume'].values\n",
        "m17_2_6  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==2) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==6) , 'salesVolume'].values\n",
        "\n",
        "m17_3_4  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==4) , 'salesVolume'].values\n",
        "m17_3_5  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==5) , 'salesVolume'].values\n",
        "m17_3_6  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==6) , 'salesVolume'].values\n",
        "m17_3_7  = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values / train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==7) , 'salesVolume'].values\n",
        "\n",
        "m16_1 = m16_1_2/2 + m16_1_3/4 + m16_1_4/8 + m16_1_5/8\n",
        "m16_2 = m16_2_3/2 + m16_2_4/4 + m16_2_5/8 + m16_2_6/8\n",
        "m16_3 = m16_3_4/2 + m16_3_5/4 + m16_3_6/8 + m16_3_7/8\n",
        "\n",
        "m17_1 = m17_1_2/2 + m17_1_3/4 + m17_1_4/8 + m17_1_5/8\n",
        "m17_2 = m17_2_3/2 + m17_2_4/4 + m17_2_5/8 + m17_2_6/8\n",
        "m17_3 = m17_3_4/2 + m17_3_5/4 + m17_3_6/8 + m17_3_7/8\n",
        "\n",
        "m1 = m16_1 * 0.4 +  m17_1 * 0.6\n",
        "m2 = m16_2 * 0.4 +  m17_2 * 0.6\n",
        "m3 = m16_3 * 0.4 +  m17_3 * 0.6\n",
        "\n",
        "evaluation_public.loc[evaluation_public.regMonth==2, 'forecastVolum'] = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==1) , 'salesVolume'].values / m1\n",
        "evaluation_public.loc[evaluation_public.regMonth==3, 'forecastVolum'] = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==2) , 'salesVolume'].values / m2\n",
        "evaluation_public.loc[evaluation_public.regMonth==4, 'forecastVolum'] = train_sales.loc[(train_sales.regYear==2017)&(train_sales.regMonth==3) , 'salesVolume'].values / m3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLFzqY0-vSpZ",
        "colab_type": "code",
        "outputId": "63745824-41e8-4b51-c496-c84983d861b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(train_sales.loc[(train_sales.regMonth<=4) ,'salesVolume'].mean())\n",
        "print(evaluation_public['forecastVolum'].mean())\n",
        "evaluation_public[['id','forecastVolum']].round().astype(int).to_csv('ccf_car_sales.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "524.1121212121212\n",
            "477.4628274411716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7G8RuNVvSps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.concat([train_sales, evaluation_public], ignore_index=True)\n",
        "data = data.merge(train_search, 'left', on=['province', 'adcode', 'model', 'regYear', 'regMonth'])\n",
        "data = data.merge(train_user, 'left', on=['model', 'regYear', 'regMonth'])\n",
        "data['label'] = data['salesVolume']\n",
        "data['id'] = data['id'].fillna(0).astype(int)\n",
        "del data['salesVolume'], data['forecastVolum']\n",
        "\n",
        "num_feat = ['adcode', 'regMonth', 'regYear', 'popularity', 'carCommentVolum', 'newsReplyVolum']\n",
        "cate_feat = ['bodyType', 'model', 'province']\n",
        "\n",
        "for i in cate_feat:\n",
        "    data[i] = data[i].astype('category')\n",
        "features = num_feat + cate_feat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLua5UPvIqp9",
        "colab_type": "code",
        "outputId": "2ee34316-914f-483d-8f68-f85a136647d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "data['bodyType'] = pd.Categorical(data.bodyType).codes\n",
        "data['province'] = pd.Categorical(data.province).codes\n",
        "data['model'] = pd.Categorical(data.model).codes\n",
        "data.head(3)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>adcode</th>\n",
              "      <th>bodyType</th>\n",
              "      <th>id</th>\n",
              "      <th>model</th>\n",
              "      <th>province</th>\n",
              "      <th>regMonth</th>\n",
              "      <th>regYear</th>\n",
              "      <th>popularity</th>\n",
              "      <th>carCommentVolum</th>\n",
              "      <th>newsReplyVolum</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>310000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>292.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>530000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>1594.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>466.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>150000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2016</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>257.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   adcode  bodyType  id  ...  carCommentVolum  newsReplyVolum  label\n",
              "0  310000         2   0  ...             11.0           106.0  292.0\n",
              "1  530000         2   0  ...             11.0           106.0  466.0\n",
              "2  150000         2   0  ...             11.0           106.0  257.0\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "188mlEx8_WnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNUX4uK__uWv",
        "colab_type": "text"
      },
      "source": [
        "不同月份 的省份所卖的总车辆、在这一年中占的比重\n",
        "\n",
        "不同月份，的车型所卖的总车辆，在这一年中占的比重\n",
        "\n",
        "不同月份，的车类型所卖的总车辆，在这一年中占的比重\n",
        "\n",
        "\n",
        "不同的季度，同上\n",
        "\n",
        "省 车型 车类型 时间\n",
        "\n",
        " 省 车类型 时间\n",
        " \n",
        " 省 车型 时间\n",
        " \n",
        " 全国 车型 时间\n",
        " \n",
        "全国 车类型 时间\n",
        "\n",
        "↑表示group by的参数。\n",
        "\n",
        "是否有月份异常需要单独拿出来训练\n",
        "\n",
        "当月是哪个季节？？【还未加入】\n",
        "\n",
        "当月是上半年还是下半年？？【还未加入】\n",
        "\n",
        "相邻年，一阶差分，一阶比值\n",
        "\n",
        "相邻月份，一阶差分，一阶比值\n",
        "\n",
        "历史月份销量比例，考虑时间衰减，月份越近占比越高*销量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXVWefir_XNq",
        "colab_type": "text"
      },
      "source": [
        "## 时间处理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKEH9odYykTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['date'] = (np.array(data.regYear.astype(str),dtype='datetime64[Y]') \n",
        " + np.array(data.regMonth-1,dtype='timedelta64[M]'))\n",
        "data['date2'] = pd.to_datetime(data['date'])\n",
        "data['date2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3CUxmZz5-94",
        "colab_type": "code",
        "outputId": "e1e58581-9800-4b18-a641-76d863241600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data['date2'].head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2016-01-01\n",
              "1       2016-01-01\n",
              "2       2016-01-01\n",
              "3       2016-01-01\n",
              "4       2016-01-01\n",
              "5       2016-01-01\n",
              "6       2016-01-01\n",
              "7       2016-01-01\n",
              "8       2016-01-01\n",
              "9       2016-01-01\n",
              "10      2016-01-01\n",
              "11      2016-01-01\n",
              "12      2016-01-01\n",
              "13      2016-01-01\n",
              "14      2016-01-01\n",
              "15      2016-01-01\n",
              "16      2016-01-01\n",
              "17      2016-01-01\n",
              "18      2016-01-01\n",
              "19      2016-01-01\n",
              "20      2016-01-01\n",
              "21      2016-01-01\n",
              "22      2016-01-01\n",
              "23      2016-01-01\n",
              "24      2016-01-01\n",
              "25      2016-01-01\n",
              "26      2016-01-01\n",
              "27      2016-01-01\n",
              "28      2016-01-01\n",
              "29      2016-01-01\n",
              "           ...    \n",
              "36930   2018-04-01\n",
              "36931   2018-04-01\n",
              "36932   2018-04-01\n",
              "36933   2018-04-01\n",
              "36934   2018-04-01\n",
              "36935   2018-04-01\n",
              "36936   2018-04-01\n",
              "36937   2018-04-01\n",
              "36938   2018-04-01\n",
              "36939   2018-04-01\n",
              "36940   2018-04-01\n",
              "36941   2018-04-01\n",
              "36942   2018-04-01\n",
              "36943   2018-04-01\n",
              "36944   2018-04-01\n",
              "36945   2018-04-01\n",
              "36946   2018-04-01\n",
              "36947   2018-04-01\n",
              "36948   2018-04-01\n",
              "36949   2018-04-01\n",
              "36950   2018-04-01\n",
              "36951   2018-04-01\n",
              "36952   2018-04-01\n",
              "36953   2018-04-01\n",
              "36954   2018-04-01\n",
              "36955   2018-04-01\n",
              "36956   2018-04-01\n",
              "36957   2018-04-01\n",
              "36958   2018-04-01\n",
              "36959   2018-04-01\n",
              "Name: date2, Length: 36960, dtype: datetime64[ns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl4Eeqxkr55P",
        "colab_type": "code",
        "outputId": "526d37a0-5f5d-449f-c575-40bfc410bab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "\n",
        "\n",
        "data['regMonth'] = data['regMonth'].apply(lambda x: int(x) )\n",
        "data['regYear'] = data['regYear'].apply(lambda x: int(x) )\n",
        "data['date'] = datetime.date.__new__(data['regYear'],data['regMonth'])\n",
        "# data['regYear'].apply(lambda x: str(x) ) +data['regMonth'].apply(lambda x: str(x) )\n",
        "\n",
        "# data['date'] =pd.to_datetime(data['date'])\n",
        "data['date']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bac6f6fd4f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regMonth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regMonth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regYear'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regYear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regYear'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regMonth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# data['regYear'].apply(lambda x: str(x) ) +data['regMonth'].apply(lambda x: str(x) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: datetime.date.__new__(X): X is not a type object (Series)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJwXO2vYRNTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_index = (data['label'].isnull()) | (data['label'] == -1)\n",
        "train_data = data[~test_index].reset_index(drop=True)\n",
        "\n",
        "train_x = train_data[features]\n",
        "train_y = train_data['label']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dglOIMybTnoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Z8nVPPQ474",
        "colab_type": "code",
        "outputId": "093a03dd-2615-48ca-ea3a-79a3bd637728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "parameters = {\n",
        "\t'num_leaves':[64], \n",
        "\t'reg_alpha':[0.], \n",
        " \t'reg_lambda':[ 2],\n",
        "   'max_depth':[-1],\n",
        "   'min_child_samples':[50],\n",
        "    'subsample':[0.7],\n",
        "     'n_estimators' : [1500],\n",
        "    'subsample_freq': [1]\n",
        " \n",
        "}\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    num_leaves=32, reg_alpha=0., reg_lambda=0.01, objective='mse', metric='mae',\n",
        "    max_depth=-1, learning_rate=0.05, min_child_samples=20,\n",
        "    n_estimators=1000, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,n_jobs=-1\n",
        ")\n",
        "\n",
        "clf = GridSearchCV( lgb_model, param_grid= parameters, scoring=\"neg_mean_squared_error\",cv=5, verbose=10 )\n",
        "clf.fit(train_x, train_y) #你和模型\n",
        "clf.best_params_\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1, score=-83915.506, total=   6.2s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1, score=-36829.321, total=   6.4s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   12.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1, score=-112038.603, total=   6.5s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   19.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1, score=-29933.613, total=   6.4s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   25.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=1, score=-85560.758, total=   6.5s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   32.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2, score=-85671.662, total=   7.9s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   39.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2, score=-37938.072, total=   7.9s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   47.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2, score=-115130.865, total=   8.0s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   55.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2, score=-30493.695, total=   8.0s\n",
            "[CV] max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  max_depth=-1, min_child_samples=50, n_estimators=1500, num_leaves=64, reg_alpha=0.0, reg_lambda=2, subsample=0.7, subsample_freq=2, score=-85413.360, total=   8.1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': -1,\n",
              " 'min_child_samples': 50,\n",
              " 'n_estimators': 1500,\n",
              " 'num_leaves': 64,\n",
              " 'reg_alpha': 0.0,\n",
              " 'reg_lambda': 2,\n",
              " 'subsample': 0.7,\n",
              " 'subsample_freq': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xifP0vtevSqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_predict_w(model, data, label='label', feature=[], cate_feature=[], random_state=2018, n_splits=5,\n",
        "#                   model_type='lgb'):\n",
        "def get_predict_w(model, data, label='label', feature=[], random_state=2018, n_splits=5,\n",
        "                  model_type='lgb'):\n",
        "    if 'sample_weight' not in data.keys():\n",
        "        data['sample_weight'] = 1\n",
        "    model.random_state = random_state\n",
        "    predict_label = 'predict_' + label\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    data[predict_label] = 0\n",
        "    test_index = (data[label].isnull()) | (data[label] == -1)\n",
        "    train_data = data[~test_index].reset_index(drop=True)\n",
        "    test_data = data[test_index]\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(train_data):\n",
        "        model.random_state = model.random_state + 1\n",
        "\n",
        "        train_x = train_data.loc[train_idx][feature]\n",
        "        train_y = train_data.loc[train_idx][label]\n",
        "\n",
        "        test_x = train_data.loc[val_idx][feature]\n",
        "        test_y = train_data.loc[val_idx][label]\n",
        "        if model_type == 'lgb':\n",
        "            try:\n",
        "                model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
        "                          eval_metric='mae',\n",
        "                          # callbacks=[lgb.reset_parameter(learning_rate=lambda iter: max(0.005, 0.5 * (0.99 ** iter)))],\n",
        "                          categorical_feature=cate_feature,\n",
        "                          sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
        "                          verbose=100)\n",
        "            except:\n",
        "                model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
        "                          eval_metric='mae',\n",
        "                          # callbacks=[lgb.reset_parameter(learning_rate=lambda iter: max(0.005, 0.5 * (0.99 ** iter)))],\n",
        "                          # categorical_feature=cate_feature,\n",
        "                          sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
        "                          verbose=100)\n",
        "        elif model_type == 'ctb':\n",
        "            model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
        "                      # eval_metric='mae',\n",
        "                      # callbacks=[lgb.reset_parameter(learning_rate=lambda iter: max(0.005, 0.5 * (0.99 ** iter)))],\n",
        "                      cat_features=cate_feature,\n",
        "                      sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
        "                      verbose=100)\n",
        "        train_data.loc[val_idx, predict_label] = model.predict(test_x)\n",
        "        if len(test_data) != 0:\n",
        "            test_data[predict_label] = test_data[predict_label] + model.predict(test_data[feature])\n",
        "    test_data[predict_label] = test_data[predict_label] / n_splits\n",
        "    #print(mse(train_data[label], train_data[predict_label]))\n",
        "    print(mse(train_data[label], train_data[predict_label]) * 5, train_data[predict_label].mean(),\n",
        "          test_data[predict_label].mean())\n",
        "\n",
        "    return pd.concat([train_data, test_data], sort=True, ignore_index=True), predict_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwsgS5IDYE5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predict_w2(model, data, label='label', feature=[], cate_feature=[], random_state=2018, n_splits=5,\n",
        "                  model_type='xgb'):\n",
        "    if 'sample_weight' not in data.keys():\n",
        "        data['sample_weight'] = 1\n",
        "    model.random_state = random_state\n",
        "    predict_label = 'predict_' + label\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    data[predict_label] = 0\n",
        "    test_index = (data[label].isnull()) | (data[label] == -1)\n",
        "    train_data = data[~test_index].reset_index(drop=True)\n",
        "    test_data = data[test_index]\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(train_data):\n",
        "        model.random_state = model.random_state + 1\n",
        "\n",
        "        train_x = train_data.loc[train_idx][feature]\n",
        "        train_y = train_data.loc[train_idx][label]\n",
        "\n",
        "        test_x = train_data.loc[val_idx][feature]\n",
        "        test_y = train_data.loc[val_idx][label]\n",
        "        if model_type == 'lgb':\n",
        "#             try:\n",
        "          model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=200,\n",
        "                    eval_metric='mae',\n",
        "                    # callbacks=[lgb.reset_parameter(learning_rate=lambda iter: max(0.005, 0.5 * (0.99 ** iter)))],\n",
        "                    categorical_feature=cate_feature,\n",
        "                    sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
        "                    verbose=100)\n",
        "#             except:\n",
        "#                 model.fit(train_x, train_y, eval_set=[(test_x, test_y)], early_stopping_rounds=100,\n",
        "#                           eval_metric='mae',\n",
        "#                           # callbacks=[lgb.reset_parameter(learning_rate=lambda iter: max(0.005, 0.5 * (0.99 ** iter)))],\n",
        "#                           # categorical_feature=cate_feature,\n",
        "#                           sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
        "#                           verbose=100)\n",
        "        elif model_type == 'xgb':\n",
        "            model.fit(train_x, train_y, eval_set=[(test_x, test_y)], \n",
        "                      #early_stopping_rounds=200,\n",
        "                       eval_metric='mae',\n",
        "                      # callbacks=[lgb.reset_parameter(learning_rate=lambda iter: max(0.005, 0.5 * (0.99 ** iter)))],\n",
        "                      #cat_features=cate_feature,\n",
        "                      sample_weight=train_data.loc[train_idx]['sample_weight'],\n",
        "                      verbose=200)\n",
        "        train_data.loc[val_idx, predict_label] = model.predict(test_x)\n",
        "        if len(test_data) != 0:\n",
        "            test_data[predict_label] = test_data[predict_label] + model.predict(test_data[feature])\n",
        "    test_data[predict_label] = test_data[predict_label] / n_splits\n",
        "    #print(mse(train_data[label], train_data[predict_label]))\n",
        "    print(mse(train_data[label], train_data[predict_label]) * 5, train_data[predict_label].mean(),\n",
        "          test_data[predict_label].mean())\n",
        "\n",
        "    return pd.concat([train_data, test_data], sort=True, ignore_index=True), predict_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHBns53Rf18A",
        "colab_type": "code",
        "outputId": "4f8e8b90-8fa0-470a-b768-67a4ad312eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "xgb_model = xgb.XGBRegressor(max_deepth =4,num_boost_round=1000)\n",
        "data, predict_label = get_predict_w2(xgb_model, data, label='label',\n",
        "                                    feature=features, cate_feature=cate_feat,\n",
        "                                    random_state=2019, n_splits=5)\n",
        "data['xgb'] = data[predict_label]\n",
        "\n",
        "data['forecastVolum'] = data['xgb'].apply(lambda x: 0 if x < 0 else x)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[08:41:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-mae:534.119\n",
            "[99]\tvalidation_0-mae:260.379\n",
            "[08:41:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-mae:548.882\n",
            "[99]\tvalidation_0-mae:268.88\n",
            "[08:41:37] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-mae:561.572\n",
            "[99]\tvalidation_0-mae:275.77\n",
            "[08:41:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-mae:544.121\n",
            "[99]\tvalidation_0-mae:261.847\n",
            "[08:41:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-mae:552.33\n",
            "[99]\tvalidation_0-mae:270.825\n",
            "1077519.25908456 608.5443903135691 219.49841039964318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OGkGzeewvSqb",
        "colab_type": "code",
        "outputId": "8601a776-c70c-49cd-f3a4-1eb630247a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lgb_model = lgb.LGBMRegressor(\n",
        "    num_leaves=32, reg_alpha=0., reg_lambda=0.01, objective='mse', metric='mae',\n",
        "    max_depth=-1, learning_rate=0.05, min_child_samples=20,\n",
        "    n_estimators=50000, subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
        ")\n",
        "\n",
        "\n",
        "data, predict_label = get_predict_w(lgb_model, data, label='label',\n",
        "                                    feature=features, \n",
        "                                    random_state=2019, n_splits=5)\n",
        "\n",
        "data['lgb'] = data[predict_label]\n",
        "\n",
        "data['forecastVolum'] = data['lgb'].apply(lambda x: 0 if x < 0 else x)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's l1: 221.065\n",
            "[200]\tvalid_0's l1: 184.071\n",
            "[300]\tvalid_0's l1: 167.892\n",
            "[400]\tvalid_0's l1: 157.69\n",
            "[500]\tvalid_0's l1: 150.949\n",
            "[600]\tvalid_0's l1: 145.815\n",
            "[700]\tvalid_0's l1: 141.164\n",
            "[800]\tvalid_0's l1: 137.39\n",
            "[900]\tvalid_0's l1: 133.801\n",
            "[1000]\tvalid_0's l1: 131.101\n",
            "[1100]\tvalid_0's l1: 128.944\n",
            "[1200]\tvalid_0's l1: 126.845\n",
            "[1300]\tvalid_0's l1: 124.857\n",
            "[1400]\tvalid_0's l1: 123.497\n",
            "[1500]\tvalid_0's l1: 121.892\n",
            "[1600]\tvalid_0's l1: 120.371\n",
            "[1700]\tvalid_0's l1: 119.234\n",
            "[1800]\tvalid_0's l1: 118.036\n",
            "[1900]\tvalid_0's l1: 117.203\n",
            "[2000]\tvalid_0's l1: 116.285\n",
            "[2100]\tvalid_0's l1: 115.652\n",
            "[2200]\tvalid_0's l1: 115.019\n",
            "[2300]\tvalid_0's l1: 113.983\n",
            "[2400]\tvalid_0's l1: 113.232\n",
            "[2500]\tvalid_0's l1: 112.48\n",
            "[2600]\tvalid_0's l1: 111.937\n",
            "[2700]\tvalid_0's l1: 111.54\n",
            "[2800]\tvalid_0's l1: 110.928\n",
            "[2900]\tvalid_0's l1: 110.412\n",
            "[3000]\tvalid_0's l1: 109.988\n",
            "[3100]\tvalid_0's l1: 109.699\n",
            "[3200]\tvalid_0's l1: 109.239\n",
            "[3300]\tvalid_0's l1: 108.958\n",
            "[3400]\tvalid_0's l1: 108.547\n",
            "[3500]\tvalid_0's l1: 108.306\n",
            "[3600]\tvalid_0's l1: 107.838\n",
            "[3700]\tvalid_0's l1: 107.479\n",
            "[3800]\tvalid_0's l1: 107.262\n",
            "[3900]\tvalid_0's l1: 106.924\n",
            "[4000]\tvalid_0's l1: 106.58\n",
            "[4100]\tvalid_0's l1: 106.241\n",
            "[4200]\tvalid_0's l1: 106.082\n",
            "[4300]\tvalid_0's l1: 105.884\n",
            "[4400]\tvalid_0's l1: 105.775\n",
            "[4500]\tvalid_0's l1: 105.531\n",
            "[4600]\tvalid_0's l1: 105.366\n",
            "[4700]\tvalid_0's l1: 105.238\n",
            "[4800]\tvalid_0's l1: 105.064\n",
            "[4900]\tvalid_0's l1: 104.926\n",
            "[5000]\tvalid_0's l1: 104.77\n",
            "[5100]\tvalid_0's l1: 104.62\n",
            "[5200]\tvalid_0's l1: 104.397\n",
            "[5300]\tvalid_0's l1: 104.301\n",
            "[5400]\tvalid_0's l1: 104.138\n",
            "[5500]\tvalid_0's l1: 104.031\n",
            "[5600]\tvalid_0's l1: 103.899\n",
            "[5700]\tvalid_0's l1: 103.77\n",
            "[5800]\tvalid_0's l1: 103.685\n",
            "[5900]\tvalid_0's l1: 103.617\n",
            "[6000]\tvalid_0's l1: 103.485\n",
            "[6100]\tvalid_0's l1: 103.418\n",
            "[6200]\tvalid_0's l1: 103.317\n",
            "[6300]\tvalid_0's l1: 103.291\n",
            "[6400]\tvalid_0's l1: 103.142\n",
            "[6500]\tvalid_0's l1: 103.049\n",
            "[6600]\tvalid_0's l1: 103.038\n",
            "[6700]\tvalid_0's l1: 102.939\n",
            "[6800]\tvalid_0's l1: 102.862\n",
            "[6900]\tvalid_0's l1: 102.78\n",
            "[7000]\tvalid_0's l1: 102.709\n",
            "[7100]\tvalid_0's l1: 102.655\n",
            "[7200]\tvalid_0's l1: 102.636\n",
            "[7300]\tvalid_0's l1: 102.555\n",
            "[7400]\tvalid_0's l1: 102.503\n",
            "[7500]\tvalid_0's l1: 102.408\n",
            "[7600]\tvalid_0's l1: 102.318\n",
            "[7700]\tvalid_0's l1: 102.244\n",
            "Early stopping, best iteration is:\n",
            "[7697]\tvalid_0's l1: 102.235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-13b815cee8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m data, predict_label = get_predict_w3(lgb_model, data, label='label',\n\u001b[1;32m      9\u001b[0m                                     \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                     random_state=2019, n_splits=5)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lgb'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-7289d0fa919e>\u001b[0m in \u001b[0;36mget_predict_w3\u001b[0;34m(model, data, label, feature, random_state, n_splits, model_type)\u001b[0m\n\u001b[1;32m     41\u001b[0m                       \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                       verbose=100)\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict_label\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, raw_score, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m                              % (self._n_features, n_features))\n\u001b[1;32m    606\u001b[0m         return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,\n\u001b[0;32m--> 607\u001b[0;31m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m         return predictor.predict(data, num_iteration,\n\u001b[1;32m   2202\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m                                  data_has_header, is_reshape)\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pred_for_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pred_for_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__pred_for_np2d\u001b[0;34m(self, mat, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__pred_for_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36minner_predict\u001b[0;34m(mat, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_parameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_num_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                 preds.ctypes.data_as(ctypes.POINTER(ctypes.c_double))))\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_preds\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mout_num_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wrong length for predict results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB61qAGLvSqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[data.label.isnull()][['id', 'forecastVolum']].round().astype(int).to_csv('ccf_car_sales082602.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StGODuRBvSq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x,train_y,test_x,test_y  = train_test_split( X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}