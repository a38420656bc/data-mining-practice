
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819230818482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70#pic_center =300x)

---------------------------

第一章是一些基础的科普和发展历史，粗粗一看略过。

## 二，神经网络基础
#### 2.1神经网络基本结构
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819223722523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_5,color_FFFFFF,t_70#pic_center =300x)
最左边的层叫作输入层，该层负责接收输入数据；最右边的层叫作输出层，我们可
以从该层获取神经网络输出数据。输入层和输出层之间的层叫作隐藏层。

其基本的组成单元是神经元，也叫做感知机。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819223859348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70#pic_center =400x)
在神经网络中，有了样本的输入和权重参数，我们就可以通过**正向传播求得输出值**。
然而，输出值一般会与预测值差异很大，这是因为网络层的权重是我们随机初始化的，
此时的网络还不具备预测分类功能。为了校正网络的权重，这时候就要利用神经网络反
向传播的算法，修正权重参数，使输出值逼近目标值。类似机器学习的优化算法，我们
通常采用反向传播的方法来更新梯度，从而达到最小化损失函数值的目的。

假设每个类别服从二项式分布，那么每个样本的目标函数写成
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819224133718.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819224200209.png#pic_center =400x)
假定我们有基于全连接的多层神经网络，每层的输出来用sigmoid 函数激活，那么
可以得到下面的递推公式：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819224219519.png#pic_center =300x)
如果对低层的网络参数求导，比如对第l 1 层的参数吟旷求导，通常采用下面的递归
求导公式
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819224330791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70#pic_center =400x)
反复采用此式，我们可以对每个参数求导，并更新所有的网络参数

#### 2.2 优化算法
##### 2.2.1初始参数

深度神经网络它是一种**选代更新算法**，需要在迭代更新前对每个参数进行初始化。不过初始化的选择在高维非凸优化问题中是非常重要的。如果初始化参数太小，在网络训练过程中前馈和反馈的信号可能会丢失，导致神经元之间没有区分。如果参数过大，可能会导致梯度失控爆炸等问题，从而影响模型的收敛性。所以，选择合适的方法初始化网络参数是非常有必要的

初始化网络参数有以下三种：

 - 高斯分布初始化
 -  均匀分布初始化
 - Xavier初始化

##### 2.2.2 学习速率的选择

学习率太小导致**收敛缓慢**，而学习率太大会**阻碍收敛并导致损失函数在最小值附近波动**或者发散。而**模拟退火**算法可以以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，达到全局的最优解。因此，深度网络通常采用模拟退火的方法在训练期间动态调整学习率。模拟退火算法的学习率又包括反向衰减学习率和指数衰减学习率，具体方法有：

- 反向衰减学习率。假设初始化学习率为咐，γ是衰减系数，t 是迭代次数，反向衰减
可以定义为
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819225023945.png#pic_center =110x)
- 指数衰减学习率类似地，指数衰减可以定义成
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819225045888.png#pic_center =120x)

> 固定衰减的模拟退火方法不能直接泛化到多个数据集上，我们也不希望采用相同 的频率和步长来更新所有的网络参数，所以自适应调整学习率Adadelta被提出，这些 方法给每个参数设置不同的自适应学习率。

- 动量方法：该方法采用**累计梯度**来替代当前时刻的梯度。直观上讲，动量方法类似
把球推下山，球在下坡时积累动力，在途中速度变得越来越快。如果某些参数在连续时
间内梯度方向不同，那么动量会变小。相反，如果在连续时间内梯度方向一致，那么动
量会增大。因此，动量法可以**更快速地收敛并减少目标函数的震荡**。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019081922524313.png#pic_center =300x)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819225300348.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70)
- RMSproop 可以自适应调整每个参数的学习率。此外，该方法也可以克服学习率过早衰减等问题 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819225338501.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/20190819225402292.png)
- 自适应矩估计

##### 2.3卷积网络
卷积网络和普通神经网络非常相似：它们都由一系列神经元组成。**但不同的是，卷
积神经网络受到视觉系统的启发，会考虑输入的空间结构**。比如，第l+ 1 层神经元和l
层的局部区域相连接，该区域（也被称为局部感受野〉执行卷积操作和非线性变换生成
第l+ l 层神经元。

卷积网络通过损失函数，比如交叉；脑来优化网络参数。此外，与标准神经网络相比，CNN 具有更少的参数，从而可以有效地训练非常深的架构（通常超过5 层，这对于全连接的网络来说是不可行的）。通常，卷积神经网络的图层可以分成三种类型：卷积层、池化层和全连接层。我们通过堆叠这些图层构成完整的卷积网络架构。

###### 2.3.1卷积层
卷积层是卷积神经网络的核心图层，用来提取**局部区域**的特征。

