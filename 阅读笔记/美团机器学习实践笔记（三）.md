## 第三章 常用模型
### 3.1 逻辑回归
逻辑回归是一种广义线性模型，它与线性回归模型包含的线性函数十分相似。但逻辑回归通过对数概率函数将线性函数的结果进行映射，目标函数的取值空间从(一∞，+∞) 映射到了(0，1) ,从而可以处理分类问题。![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826185640828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70)

我们将线性回归与逻辑回归进行对比，可以发现线性回归模型在训练时在整个实数域上对异常点的敏感性是一致的(见图3-1 )，因而在处理分类问题时线性回归模型的效果较差，线性回归
模型不适合处理分类问题。对于二分类任务，逻辑回归输出标记y ε{0 ，1} ，而线性回归模型产生的预测值y= eTx是实值，所以我们需要一个映射函数将实值转换为0-1的值。

**(逻辑回归模型) 二项逻辑回归模型有如下的条件概率分布**![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826185723715.png)
使用梯度下降法对逻辑回归进行求解的过程：
令![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826185823508.png)
构造最大似然概率:
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826185855270.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826185906840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70)
#### 3.1.2 应用
业界对逻辑回归的研究热点主要集中在稀疏性、准确性和大规模计算上。实际应用逻辑回归前，经常会对特征进行独热(One Hot) 编码，比如广告点击率应用中的用户ID 、广告ID 
#### 补充：实践

```
from  sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
data=pd.read_csv(xxx)
X_train,X_test,y_train,y_test=train_test_split(data[column_names[1:10]],data[column_names[10]],test_size=0.25,random_state=33) 
ss = StandardScaler()
X_train = ss.fit_transform(X_train)
X_test=ss.transform(X_test)
lr = LogisticRegression()   # 初始化LogisticRegression
lr.fit(X_train,y_train)   # 使用训练集对测试集进行训练
lr_y_predit=lr.predict(X_test)  # 使用逻辑回归函数对测试集进行预测
print ('Accuracy of LR Classifier:%f'%lr.score(X_test,y_test))  
```
### 3.2 场感知因子分解机
逻辑回归**无法学习到特征间的组合关系**，而特征组合关系在推荐和CTR预估中却是比较常见的。在进行点击率预估时，特征通常来自于用户、广告和上下文环境，如果没有对这些特征进行组合，模型就无法学习到所有有用的信息。

例如，同一个用户在不同时间或者地点感兴趣的广告是不同的;同一件商品在不同地区的受欢迎程度也是不同的。但人工对特征组合需要做大量的特征工程工作，对特征做暴力组合模型又太复杂、参数太多。模型训练迭代无论是内存开销还是时间开销都让人很难接受，迭代效果往往也比较差。**有没有可以自动做特征组合，并且算法效率比较高的模型?这就是本节要介绍的因子分解机和场感知因子分解机**

#### 3.2.1 因子分解机原理
设特征维数为n ，则二次项的参数数目为n(n+l)/2，特别是某些广告ID 、用户ID类特征，其特征维数可能达到几百万维，这导致只有极少数的二阶组
合模式才能找到，所以这些特征组合后得到的特征矩阵就是十分稀疏。而在训练样本不足的时候，特征矩阵的稀疏性很容易导致相关参数准确性较低，导致模型效果不好。而我们可以通过对二次项参数施加某种限制来减少参数的自由度。
通过观察因子分解机模型我们可以发现，
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826190815318.png)
这虽然对特征的计算进行了非常大的简化，但在实际预测中，特征间往往有巨大的差异，因子分解机任意两组特征交叉的隐向量都是相关的，这实际上限制了模型的复杂度。可是如果任意一对特征组合都是完全独立的，这与之前提到的通过支持向量机核函数来计算特征交叉类似，它们有着极高的复杂性和自由度，模型计算十分复杂，效果也不明显。可以通过引人特征组(场)的概念来优化这个问题。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826190914902.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70)
#### 3.2.3 应用
场感知因子分解机可以自动做特征组合和处理高维稀疏特征，因而它在处理大量离散特征的问题上往往有比较好的效果。使用场感知因子分解机时要注意对连续特征做归一化或离散化。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190826191045351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70)
### 3.3 梯度提升树
梯度提升树模型是一种基于回归树的集成学习方法，它通过构造多个弱的回归树作为基学习器，并把这些树的结果累加起来作为最终的预测输出。因为其特征处理的简单性和优异的效果，梯度提升树模型被认为是传统统计学习中效果最好的方法之一。目前广泛使用的开源工具XGBoost和LightGBM ，它们都是梯度提升树非常优秀的实现。


