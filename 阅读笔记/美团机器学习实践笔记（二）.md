## 特征工程
>据和特征决定了机器学习算法的上限，而模型和算法只是
不断逼近这个上限而已

### 2.1 特征提取
根据机器学习算法所要学习的目标和业务逻辑，我们需要考虑数据中有哪些可能相关的要素。
例如在美团酒店搜索排序中，酒店的销量、价格、用户的消费水平等是强相关的因素，用户的年龄、位置可能是弱相关的因素，用户的D是完全无关的因素。在确定了哪些因素可能与预测目标相关后，我们需要将此信息表示为数值类型，即为特征抽取的过程。

#### 2.1.1 探索性数据分析
在统计学里，探索性数据分析( Expl orat。可Data Analysis , **EDA**) 是采用各种技术(大部分为可视化技术)在尽量少的先验假设条件下，探索数据内部结构和规律的一种数据分析方法或理念。

EDA技术通常可分为两类。一类是**可视化技术**，如箱形图、直方图、多变量图、链图、|怕累托图、散点图、茎叶图、平行坐标、让步比、多维尺度分析、目标投影追踪、主成分分析、多线性主成分分析、降维、非线性降维等;另一类是**定量技术**，如样本均值、方差、分位数、峰度、偏度等。

#### 2.1.2 数值特征
数值类型的数据具有实际测量意义，例如人的身高、体重、Jfrl压等，或者是计数，例如一个网站被浏览多少次、一种产品被购买多少次等(统计学家也称数值类型的数据为定量数据)。数值类型的数据可以分为离散型和连续型。

机器学习模型可以直接将数值类型的数据格式作为输入，但这并不意味着没有必要进行特征工程。好的特征不仅能表示出数据中隐藏的关键信息，而且还与模型的假设一致。通常情况下，对数值类型的数据进行适当的数值变换能带来不锚的效果提升。

对于数值特征，我们主要考虑的因素是它的大小和分布。对于那些目标变量为输入特征的光滑函数的模型，如线性回归、逻辑回归等，其对输入特征的大小很敏感。因此，当使用光滑函数建模时，有必要对输入进行归一化。
而对于那些基于树的模型，例如随机森林、梯度提升树等，其对输入特征的大小不敏感，输入不需要进行归一化。但是，对于树模型，如果特征取值无限大也会有问题。如果模型对输入特征和目标变量有一些隐式或者显式的假设，则数据的分布对模型很重要。

- 截断 ：对于连续型数值特征，有时候:太多的精度可能只是噪声。因此，可以在保留重要信息的前提下对特征进行截断，截断后的特征也可以看作是类别特征。另外，至于长尾的数据，可以先进行对数转换，然后进行截断。
- 二值化 ：数值特征的一种常用类型是计数特征，如网站每天的访问量、餐厅的评论数、用户对一首歌的播放次数等。在大数据时代，计数可以非常快地累加。处理计数特征时，首先要考虑的是，保留为原始计数还是**转换为二值变量**来标识是否存在或者进行分桶操作。
- 分桶 ：一个特征对应一个系数，从而模型往往只对比较大的特征值敏感。对于这种情况，通常的解决方法是进行分桶。分桶是将数值变量分到一个桶里并分配一个桶编号，常见的分桶方法有固定宽度的分桶。
- 缩放
- 缺失值处理：对于特征缺失，我们有两类处理方
法。第一种是补一个值，例如最简单的方法是补一个均值;对于包含异常值的变量，更加健壮一些的方法则是补一个中位数;除此之外还可以使用模型预测缺失值。另外一种则是直接忽略，即将缺失作为一种信息进行编码喂给模型让其进行学习。现在有一些模型可以直接处理缺失值，例如XGBoost模型可以处理缺失特征
- 特征交叉
- 线性编码

#### 2.1.3 类别特征
- 自然数编码
- 独热编码
- 分层编码
- ...
#### 2.1.4 时间特征
时间变量可以直接作为类别变量处理，类别特征的处理方式对于时间特征同样适用。但时间变量还包含其他更加丰富的信息。时间变量常用的表达方式有年、月、日、时\分、利'\星期几，以及一年过了多少天、一天过了多少分钟、季度、是否闰年、是否季度初、是否季度末、是否月初、是否月末、是
否周末，还有是否营业时间、是否节假日等。除了对单个时间变量的预处理之外，根据具体业务对两个时间变量之间进行组合也能提取重要的特征。例如可以计算产品上线到现在经过了多长时间，顾客上次借款距离现在的时间间隔，两个时间间隔之间是否包含节假日或其他特殊日期等。

除了上面提到的基于时间本身的特征之外，**时间变量更重要的是时间序列相关的特征**。时间序列不仅包含一维时间变量，还有一维其他变量，如股票价格、天气温度、降雨量、订单量等。时间序列分析的主要目的是基于历史数据来预测未来信息。

#### 2.1.6 文本特征‘
我们可以从以下几个方面对文本特征进行预处理: 将字符转化为小写、分词、
去除无用字符、提取词根、拼写纠错、词干提取、标点符号编码、文档特征、实体插入和提取、Word2Vec 、文本相似性、去除停止词、去除稀有词、TF-IDF ，LDA 、LSA等。
- 语料构建 ：构建一个由文档或短语组成的矩阵。矩阵的每一行为文档，可以理解为对产品的描述，每一列为单词。通常，文挡的个数与样本个数一致。
- 文本清洗：如果数据通过网页抓取，首先剔除文本中的HTMU示记;停止词只用于语句的构建，但不包含任何真实的信息，因此需要剔除;为了避免文本中的大小写差异，整个文本通常需要转换为小写形式; 统-编码;去除标点符号; 去除数字; 去除空格; 还原为词根。但是在某些情况下，文本不一定需要进行清洗，这取决于具体的应用场景。
- 词性标注：词语通常有三类重要的词性: 名词、动词和形容词。名词特指人、动物、概念或事物，动词表达动作，形容词描述了名词的属性。词性标注的目标是为文本中的每个词标注一个合适的词性。
- 词形还原
- 文本统计特征：文本的长度、单词个数、数字个数、字母个数、大小写单词个数、大小写字母个数、标点符号个数、特殊字符个数等，数字占比、字母占比、特殊字符占比等，以及名词个数、动词个数等。
- N-Gram模型：在自然语言处理中，N-Gram模型将文本转换为连续序列，序列的每一项包含n个元素(可以是单词或者字母等元素) ，例如"the dog smelled like askunk" ，得到3-Gram ( the dog smelled , dog smeJled like , smelled like a , like a s阳nk )。这种想法是为了将一个或者两个甚至多个单词同时出现的信息喂给模型。为了更好地保留词序信息，构建更有效的语言模型，我们希望在N-Gram模型中选用更大的η。但是当n很大时，数据会很稀疏。3-Gram是常用的选择。统t忖吾言模型一般都是基于N-Gram 的统计估计条件概率，基于神经网络的语-言模型也是对N-Gram进行建模。
###### Skip-Gram模型
- 词集模型：机器学习模型不能直接处理文本，因此我们需要将文本(或者N-Gram序列)转化为实数或者实向量。在**词集模型中，向量的每个分量的取值为0和1 ，代表单同是否在文档中出现。**向量空间模型没有考虑词序信息。
- 词袋模型：在词集模型中，向量的取值不考虑单词出现的次数，这会损失很多信息。词袋模型中，向量的每个分量的取值为单词在文档中的词频，为了避免向量的维度太大，通常会过滤掉在文档集合中词频很小的单词。
- TD-IDF：TF (Term Frequency ，词频)、IDF ( Inverse Document Frequency ，逆文档频率)，用来评估单词对于文件集或语料库中的其中一份文件的重要程度。单词或短语的重要性随着它在文件中出现的次数成正比增加，同时随着它在语料库中出现的频率成反比下降。基于TF-IDF和词袋模型得到的表示文本的向量往往维度非常大，因此实际应用中一般需要降维处理。
- 余弦相似度
- Jaccard相似度
- Word2Vec：Word2Vec是最常用的一种单词嵌入，即将单词所在的空间(高维空间)映射到一个低维的向量空间中，这样每个单词对应一个向量，通过计算向量之间的余弦相似度就可以得到某个单词的同义词。传统的单词表示，如独热编码，仅仅是将词转化为数字表示，不包含任何语义信息。而单词嵌入包含了单词的语义信息，这类表示称为分布式表示。


### 2.2.4 小结
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190822194838536.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RlbmdjaGVuZ3R1NDEzOQ==,size_16,color_FFFFFF,t_70)


